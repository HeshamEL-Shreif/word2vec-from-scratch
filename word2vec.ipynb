{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# install dependancies"
      ],
      "metadata": {
        "id": "8Z4mYdWV9ob4"
      },
      "id": "8Z4mYdWV9ob4"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "id": "0PYiayz64eL-"
      },
      "id": "0PYiayz64eL-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "z9sjmSFX9s4D"
      },
      "id": "z9sjmSFX9s4D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b90d4f1",
      "metadata": {
        "id": "3b90d4f1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from huggingface_hub import notebook_login\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "AYHQS2vL44tV"
      },
      "id": "AYHQS2vL44tV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "lHMsqXxw9_Yb"
      },
      "id": "lHMsqXxw9_Yb"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"ag_news\")"
      ],
      "metadata": {
        "id": "_jQ9yJ_P4d1N"
      },
      "id": "_jQ9yJ_P4d1N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d102a35",
      "metadata": {
        "id": "9d102a35"
      },
      "outputs": [],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get a subset aas the corpus"
      ],
      "metadata": {
        "id": "TewTbiaV-CjB"
      },
      "id": "TewTbiaV-CjB"
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = dataset[\"train\"][\"text\"][:5000]"
      ],
      "metadata": {
        "id": "ENZbJYgo6f8i"
      },
      "id": "ENZbJYgo6f8i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Tokenizer"
      ],
      "metadata": {
        "id": "KJHaSauV-GeT"
      },
      "id": "KJHaSauV-GeT"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = WordLevelTrainer(vocab_size=5000, special_tokens=[\"[UNK]\", \"[PAD]\"])"
      ],
      "metadata": {
        "id": "Tx8_ZJzR6i0W"
      },
      "id": "Tx8_ZJzR6i0W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train_from_iterator(corpus, trainer)"
      ],
      "metadata": {
        "id": "tNFoLBF96vui"
      },
      "id": "tNFoLBF96vui",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(corpus[0])\n",
        "print(output.tokens)\n",
        "print(output.ids)"
      ],
      "metadata": {
        "id": "79wkHRHV64SR"
      },
      "id": "79wkHRHV64SR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = [tokenizer.encode(sentence).ids for sentence in corpus]"
      ],
      "metadata": {
        "id": "-jX1G-4v7wWW"
      },
      "id": "-jX1G-4v7wWW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_text[0])"
      ],
      "metadata": {
        "id": "jmLLBxz3700D"
      },
      "id": "jmLLBxz3700D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenizer.get_vocab()"
      ],
      "metadata": {
        "id": "iU7_rs7b-laj"
      },
      "id": "iU7_rs7b-laj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Skip-Gram Pairs"
      ],
      "metadata": {
        "id": "agPLV60S-LxJ"
      },
      "id": "agPLV60S-LxJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_skip_gram_pairs(tokenized_text, window_size):\n",
        "    pairs = []\n",
        "    for sentence in tokenized_text:\n",
        "        for i in range(len(sentence)):\n",
        "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                if i != j:\n",
        "                    pairs.append((sentence[i], sentence[j]))\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "tHD_OcZr6_xB"
      },
      "id": "tHD_OcZr6_xB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = get_skip_gram_pairs(tokenized_text, 2)"
      ],
      "metadata": {
        "id": "0_HnlsMe8CAF"
      },
      "id": "0_HnlsMe8CAF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pairs[0:10])"
      ],
      "metadata": {
        "id": "BQWagsRR8ER2"
      },
      "id": "BQWagsRR8ER2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-gram dataset"
      ],
      "metadata": {
        "id": "O6Ytex0X-O1e"
      },
      "id": "O6Ytex0X-O1e"
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram_Data(Dataset):\n",
        "  def __init__(self, pairs):\n",
        "    self.pairs = pairs\n",
        "  def __len__(self):\n",
        "    return len(self.pairs)\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.pairs[idx][0]), torch.tensor(self.pairs[idx][1])"
      ],
      "metadata": {
        "id": "dwdXE-xm8IOX"
      },
      "id": "dwdXE-xm8IOX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get negative samples"
      ],
      "metadata": {
        "id": "pwRfMrh6-S8z"
      },
      "id": "pwRfMrh6-S8z"
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_negative_words(batch_size, vocab_size, num_negatives, device, true_context):\n",
        "    negative_samples = []\n",
        "    for _ in range(batch_size):\n",
        "        negatives = []\n",
        "        while len(negatives) < num_negatives:\n",
        "            rand_word = random.randint(0, vocab_size - 1)\n",
        "            if rand_word != true_context[_].item():\n",
        "                negatives.append(rand_word)\n",
        "        negative_samples.extend(negatives)\n",
        "    return torch.tensor(negative_samples, device=device)"
      ],
      "metadata": {
        "id": "jRUVa_ds8l_m"
      },
      "id": "jRUVa_ds8l_m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Class"
      ],
      "metadata": {
        "id": "hDQ7u7aG-Z0H"
      },
      "id": "hDQ7u7aG-Z0H"
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, center_words, context_words):\n",
        "        center_embed = self.in_embed(center_words)\n",
        "        context_embed = self.out_embed(context_words)\n",
        "        scores = torch.sum(center_embed * context_embed, dim=1)\n",
        "        return scores\n",
        "\n",
        "    def get_embeddings(self):\n",
        "        return self.in_embed.weight.data"
      ],
      "metadata": {
        "id": "wqqo5MUq955m"
      },
      "id": "wqqo5MUq955m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initiate dataset"
      ],
      "metadata": {
        "id": "w2-e0pbCCB5r"
      },
      "id": "w2-e0pbCCB5r"
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SkipGram_Data(pairs)\n",
        "dataloader = DataLoader(train_dataset, batch_size=4096, shuffle=True)"
      ],
      "metadata": {
        "id": "KckRwRS48fzX"
      },
      "id": "KckRwRS48fzX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jbTQfq23CIcc"
      },
      "id": "jbTQfq23CIcc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set device"
      ],
      "metadata": {
        "id": "vsgFOBmSCI3u"
      },
      "id": "vsgFOBmSCI3u"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "GMqq5upp-g-L"
      },
      "id": "GMqq5upp-g-L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "3BIDigTvCK92"
      },
      "id": "3BIDigTvCK92"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "num_negatives = 5\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "model = Word2Vec(vocab_size, embedding_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "-7M91AubCPDX"
      },
      "id": "-7M91AubCPDX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    dataloader_loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for center, pos_context in dataloader_loop:\n",
        "        center = center.to(device)\n",
        "        pos_context = pos_context.to(device)\n",
        "        batch_size = center.size(0)\n",
        "\n",
        "        pos_scores = model(center, pos_context)\n",
        "        pos_labels = torch.ones_like(pos_scores)\n",
        "\n",
        "        neg_context = sample_negative_words(batch_size, vocab_size, num_negatives, device, pos_context)\n",
        "        neg_center = center.repeat_interleave(num_negatives)\n",
        "        neg_scores = model(neg_center, neg_context)\n",
        "        neg_labels = torch.zeros_like(neg_scores)\n",
        "\n",
        "        scores = torch.cat([pos_scores, neg_scores])\n",
        "        labels = torch.cat([pos_labels, neg_labels])\n",
        "        loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        dataloader_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {(total_loss/len(dataloader)):.4f}\")"
      ],
      "metadata": {
        "id": "7awzatQj99__"
      },
      "id": "7awzatQj99__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "W1_ua5H_HxBt"
      },
      "id": "W1_ua5H_HxBt"
    },
    {
      "cell_type": "code",
      "source": [
        "test_corpus = dataset[\"test\"][\"text\"][:100]"
      ],
      "metadata": {
        "id": "xpWX2LAeH0vd"
      },
      "id": "xpWX2LAeH0vd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_corpus[0])"
      ],
      "metadata": {
        "id": "P-xBE7zSICNv"
      },
      "id": "P-xBE7zSICNv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test_text = [tokenizer.encode(sentence).ids for sentence in test_corpus]"
      ],
      "metadata": {
        "id": "eFvB3tUAIEcK"
      },
      "id": "eFvB3tUAIEcK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pairs = get_skip_gram_pairs(tokenized_test_text, 2)"
      ],
      "metadata": {
        "id": "_n8mc0XbIIhI"
      },
      "id": "_n8mc0XbIIhI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SkipGram_Data(test_pairs)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "1LfVQIIZIL4u"
      },
      "id": "1LfVQIIZIL4u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    total_loss = 0\n",
        "    dataloader_loop = tqdm(test_dataloader, desc=f\"test\", leave=False)\n",
        "\n",
        "    for center, pos_context in dataloader_loop:\n",
        "        center = center.to(device)\n",
        "        pos_context = pos_context.to(device)\n",
        "        batch_size = center.size(0)\n",
        "\n",
        "        pos_scores = model(center, pos_context)\n",
        "        pos_labels = torch.ones_like(pos_scores)\n",
        "\n",
        "        neg_context = sample_negative_words(batch_size, vocab_size, num_negatives, device, pos_context)\n",
        "        neg_center = center.repeat_interleave(num_negatives)\n",
        "        neg_scores = model(neg_center, neg_context)\n",
        "        neg_labels = torch.zeros_like(neg_scores)\n",
        "\n",
        "        scores = torch.cat([pos_scores, neg_scores])\n",
        "        labels = torch.cat([pos_labels, neg_labels])\n",
        "        loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        dataloader_loop.set_postfix(loss=loss.item())\n",
        "    print(f\"Average Loss = {total_loss / len(test_dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "5TaMS2Fw-izp"
      },
      "id": "5TaMS2Fw-izp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "O_Mihke-KedO"
      },
      "id": "O_Mihke-KedO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the state dictionary"
      ],
      "metadata": {
        "id": "3Pq_fsqYKxlt"
      },
      "id": "3Pq_fsqYKxlt"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"word2vec_model_state_dict.pth\")"
      ],
      "metadata": {
        "id": "c8aDKSFeIt-C"
      },
      "id": "c8aDKSFeIt-C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the entire model"
      ],
      "metadata": {
        "id": "SHHFg9o3K0m4"
      },
      "id": "SHHFg9o3K0m4"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"word2vec_full_model.pth\")"
      ],
      "metadata": {
        "id": "6f18OnN4Km3g"
      },
      "id": "6f18OnN4Km3g",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}